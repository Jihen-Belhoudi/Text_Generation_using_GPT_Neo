{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jihen-Belhoudi/Text_Generation_using_GPT_Neo/blob/main/Text_Generation_using_GPT_Neo_1_3B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ECzl0c52ivR"
      },
      "source": [
        "#GPT-Neo 1.3B\n",
        "\n",
        "**Model Description:**\n",
        "GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.\n",
        "\n",
        "**Training data:**\n",
        "GPT-Neo 1.3B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.\n",
        "\n",
        "**Training procedure:**\n",
        "This model was trained on the Pile for 380 billion tokens over 362,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJYgX-zH3Igf"
      },
      "source": [
        "The steps of our work:\n",
        "\n",
        "1. **Token Probability Distribution**: After processing the initial text, the model generates a probability distribution over the entire vocabulary for the next token. This distribution is typically obtained by passing the model's output (logits) through a softmax function, which converts the logits into probabilities.\n",
        "\n",
        "2. **Sampling Strategy**: Once the probability distribution is obtained, there are several strategies for selecting the next token:\n",
        "\n",
        "    a. **Greedy Sampling (top_k = 1)**: This strategy simply selects the token with the highest probability as the next token. It's quick and easy but might lead to repetitive or less diverse outputs.\n",
        "\n",
        "    b. **Top-k Sampling (top_k > 1)**: In this approach, the model considers only the top-k tokens with the highest probabilities and samples from this reduced set. This allows for some level of diversity while still maintaining control over the selection process.\n",
        "\n",
        "    c. **Temperature Scaling**: Another technique involves using a temperature parameter during sampling. This parameter controls the steepness of the softmax function, affecting the diversity of the generated text. Higher temperatures lead to more randomness in the sampling process, while lower temperatures make the sampling more deterministic.\n",
        "\n",
        "    d. **Random Sampling**: This strategy involves sampling directly from the entire probability distribution, where each token's probability serves as its likelihood of being chosen. This approach allows for the greatest level of diversity in the generated text.\n",
        "\n",
        "3. **Optimal Next Word**: The optimal next word is subjective and depends on the specific task and desired outcomes. While greedy sampling might produce coherent text quickly, it could lack diversity. On the other hand, random sampling could lead to more creative and diverse outputs but might sacrifice coherence.\n",
        "\n",
        "4. **Balancing Exploration and Exploitation**: Choosing the appropriate sampling strategy involves balancing exploration (seeking new, diverse outputs) and exploitation (leveraging known high-probability tokens for coherent text generation). The choice often depends on the application and desired trade-offs between novelty and coherence in the generated text.\n",
        "\n",
        "In summary, the model's ability to find the optimal next word depends on the sampling strategy employed, which balances exploration and exploitation to generate diverse yet coherent text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BguxTp_C5JQK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS0MbErn3cyn",
        "outputId": "1571e70d-8cfe-4f70-c170-5fcce2853194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# Check if GPU is available, set device accordingly\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "\n",
        "# Load the tokenizer from the pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the model from the pretrained model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqtIRypl-l-h"
      },
      "source": [
        "# Top-k Sampling (top_k > 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfC6egYw5Bak",
        "outputId": "397376a5-bdf6-46d1-ead1-a0aa85c2e8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Input             Choice 1  \\\n",
            "0                                        I am working          on (67.52%)   \n",
            "1                                     I am working on           a (49.95%)   \n",
            "2                                   I am working on a     project (12.61%)   \n",
            "3                           I am working on a project        that (26.01%)   \n",
            "4                      I am working on a project that    requires (15.65%)   \n",
            "5             I am working on a project that requires           a (14.90%)   \n",
            "6           I am working on a project that requires a          lot (5.85%)   \n",
            "7       I am working on a project that requires a lot          of (97.73%)   \n",
            "8    I am working on a project that requires a lot of         data (3.91%)   \n",
            "9   I am working on a project that requires a lot ...          to (19.17%)   \n",
            "10  I am working on a project that requires a lot ...          be (86.13%)   \n",
            "11  I am working on a project that requires a lot ...      stored (11.73%)   \n",
            "12  I am working on a project that requires a lot ...          in (37.95%)   \n",
            "13  I am working on a project that requires a lot ...           a (42.72%)   \n",
            "14  I am working on a project that requires a lot ...    database (37.99%)   \n",
            "15  I am working on a project that requires a lot ...           . (67.79%)   \n",
            "16  I am working on a project that requires a lot ...           I (26.14%)   \n",
            "17  I am working on a project that requires a lot ...          am (27.77%)   \n",
            "18  I am working on a project that requires a lot ...       using (30.31%)   \n",
            "19  I am working on a project that requires a lot ...          the (9.80%)   \n",
            "20  I am working on a project that requires a lot ...       Entity (8.17%)   \n",
            "21  I am working on a project that requires a lot ...   Framework (83.97%)   \n",
            "22  I am working on a project that requires a lot ...          to (13.60%)   \n",
            "23  I am working on a project that requires a lot ...       store (21.32%)   \n",
            "24  I am working on a project that requires a lot ...         the (49.14%)   \n",
            "25  I am working on a project that requires a lot ...        data (90.41%)   \n",
            "26  I am working on a project that requires a lot ...           . (51.87%)   \n",
            "27  I am working on a project that requires a lot ...           I (31.79%)   \n",
            "28  I am working on a project that requires a lot ...        have (36.42%)   \n",
            "29  I am working on a project that requires a lot ...           a (21.49%)   \n",
            "30  I am working on a project that requires a lot ...       class (13.57%)   \n",
            "31  I am working on a project that requires a lot ...      called (34.67%)   \n",
            "32  I am working on a project that requires a lot ...         User (7.50%)   \n",
            "33  I am working on a project that requires a lot ...        that (21.58%)   \n",
            "34  I am working on a project that requires a lot ...         has (29.49%)   \n",
            "35  I am working on a project that requires a lot ...           a (43.53%)   \n",
            "36  I am working on a project that requires a lot ...    property (17.26%)   \n",
            "37  I am working on a project that requires a lot ...      called (76.96%)   \n",
            "38  I am working on a project that requires a lot ...        User (20.68%)   \n",
            "39  I am working on a project that requires a lot ...        Name (39.69%)   \n",
            "40  I am working on a project that requires a lot ...           . (43.69%)   \n",
            "41  I am working on a project that requires a lot ...           I (32.31%)   \n",
            "42  I am working on a project that requires a lot ...          am (24.67%)   \n",
            "43  I am working on a project that requires a lot ...       using (24.56%)   \n",
            "44  I am working on a project that requires a lot ...         the (32.20%)   \n",
            "45  I am working on a project that requires a lot ...        User (10.31%)   \n",
            "46  I am working on a project that requires a lot ...        Name (26.86%)   \n",
            "47  I am working on a project that requires a lot ...    property (52.98%)   \n",
            "48  I am working on a project that requires a lot ...          to (54.35%)   \n",
            "49  I am working on a project that requires a lot ...       store (30.71%)   \n",
            "\n",
            "                  Choice 2             Choice 3             Choice 4  \\\n",
            "0            with (15.59%)           in (4.59%)           to (2.11%)   \n",
            "1               an (9.26%)          the (6.49%)           my (4.34%)   \n",
            "2              new (3.37%)        small (1.66%)          web (1.65%)   \n",
            "3           where (10.06%)          for (9.53%)           to (6.30%)   \n",
            "4        involves (11.75%)         will (9.97%)           is (9.71%)   \n",
            "5              me (14.80%)         the (11.07%)           an (2.63%)   \n",
            "6           custom (2.20%)         user (2.18%)     database (2.03%)   \n",
            "7             more (0.73%)         from (0.30%)           \\n (0.13%)   \n",
            "8           custom (1.68%)    different (1.36%)     database (1.14%)   \n",
            "9    manipulation (16.05%)            . (8.72%)          and (3.39%)   \n",
            "10        populate (1.19%)         work (0.69%)        store (0.53%)   \n",
            "11          loaded (6.12%)      entered (3.01%)         sent (2.81%)   \n",
            "12              . (19.68%)         and (11.46%)           on (6.73%)   \n",
            "13            the (11.92%)           an (6.33%)       memory (4.30%)   \n",
            "14             SQL (8.43%)        MySQL (5.74%)   relational (3.62%)   \n",
            "15               , (9.98%)          and (6.00%)        table (2.09%)   \n",
            "16            The (13.43%)              (8.02%)           \\n (7.23%)   \n",
            "17           have (26.45%)         want (5.02%)        would (4.12%)   \n",
            "18          trying (7.80%)      looking (5.14%)    currently (4.66%)   \n",
            "19               a (9.71%)          SQL (9.14%)       Entity (8.11%)   \n",
            "20       following (6.55%)          SQL (2.86%)    Microsoft (1.87%)   \n",
            "21       framework (8.47%)         Fram (4.61%)    framework (0.29%)   \n",
            "22            and (12.00%)           4 (10.89%)            6 (9.56%)   \n",
            "23             do (12.40%)       create (6.45%)        model (5.74%)   \n",
            "24           this (16.25%)           my (7.97%)        these (5.67%)   \n",
            "25     information (2.01%)     database (1.95%)     entities (0.46%)   \n",
            "26              , (11.35%)          in (10.87%)         and (10.51%)   \n",
            "27             \\n (15.29%)         The (11.17%)           My (3.48%)   \n",
            "28             am (29.58%)         want (5.35%)         need (3.13%)   \n",
            "29         created (8.14%)          the (6.08%)         been (5.68%)   \n",
            "30          table (10.06%)        model (4.00%)      problem (3.81%)   \n",
            "31           that (22.64%)          for (5.06%)        which (4.26%)   \n",
            "32          Person (5.10%)     Employee (3.91%)     Customer (3.57%)   \n",
            "33          which (16.29%)            , (8.15%)          and (6.52%)   \n",
            "34       contains (19.37%)           is (8.06%)            I (6.29%)   \n",
            "35      properties (9.30%)          the (6.83%)           an (4.86%)   \n",
            "36            list (7.87%)   collection (6.48%)          lot (5.95%)   \n",
            "37              of (6.01%)         that (3.21%)        named (2.85%)   \n",
            "38            Name (8.21%)        Email (5.76%)        First (4.35%)   \n",
            "39             Id (23.42%)          ID (20.50%)            _ (1.67%)   \n",
            "40           that (14.74%)       which (14.21%)         and (10.61%)   \n",
            "41           This (12.31%)          \\n (11.48%)         The (11.37%)   \n",
            "42           have (23.39%)        want (16.55%)         also (8.64%)   \n",
            "43         trying (21.67%)      storing (8.94%)         also (3.89%)   \n",
            "44              a (20.71%)         this (7.67%)           an (4.30%)   \n",
            "45      following (10.19%)       Entity (4.81%)      default (2.84%)   \n",
            "46              . (24.11%)       class (16.87%)     property (2.85%)   \n",
            "47             to (14.75%)           as (7.94%)           in (7.01%)   \n",
            "48             in (23.89%)           as (5.18%)          for (3.30%)   \n",
            "49          create (6.72%)     retrieve (4.86%)         save (3.81%)   \n",
            "\n",
            "              Choice 5             Choice 6           Choice 7  \\\n",
            "0      through (1.54%)           at (0.85%)        for (0.82%)   \n",
            "1         this (1.75%)         some (1.59%)    getting (1.37%)   \n",
            "2         game (1.44%)      website (1.21%)          C (1.09%)   \n",
            "3        which (6.12%)          and (5.92%)         in (5.45%)   \n",
            "4            I (8.40%)          has (6.58%)       uses (5.93%)   \n",
            "5           to (2.09%)         some (1.71%)       that (1.66%)   \n",
            "6        large (1.70%)          web (1.64%)     server (1.27%)   \n",
            "7           in (0.10%)           to (0.08%)          ( (0.05%)   \n",
            "8         code (1.02%)       memory (1.00%)    complex (0.99%)   \n",
            "9            , (3.37%)   processing (2.59%)         in (2.57%)   \n",
            "10        load (0.49%)          get (0.48%)       come (0.45%)   \n",
            "11   retrieved (2.57%)    displayed (2.51%)        fet (2.50%)   \n",
            "12           , (5.87%)          for (2.28%)       into (1.95%)   \n",
            "13         SQL (3.27%)           my (2.05%)        one (1.45%)   \n",
            "14       table (2.55%)       single (2.41%)       file (1.96%)   \n",
            "15           ( (1.57%)          for (1.21%)       that (1.13%)   \n",
            "16        This (3.49%)           It (2.64%)         My (2.25%)   \n",
            "17        need (4.03%)           'm (2.83%)        was (2.77%)   \n",
            "18         not (4.19%)     planning (3.24%)    working (2.19%)   \n",
            "19       MySQL (4.00%)         Post (3.09%)        PHP (2.78%)   \n",
            "20           M (1.74%)        MySQL (1.70%)          C (1.48%)   \n",
            "21        Data (0.25%)            - (0.17%)      Frame (0.13%)   \n",
            "22           . (6.47%)          for (4.92%)          , (3.67%)   \n",
            "23      handle (4.81%)       access (4.23%)    connect (3.78%)   \n",
            "24        data (4.54%)           it (3.48%)        and (3.15%)   \n",
            "25     objects (0.42%)      records (0.30%)     tables (0.28%)   \n",
            "26         but (3.61%)         into (1.81%)        for (1.08%)   \n",
            "27             (3.41%)      However (3.29%)       This (2.05%)   \n",
            "28       would (2.94%)          was (1.46%)       also (1.36%)   \n",
            "29         two (3.17%)           an (2.61%)    already (1.94%)   \n",
            "30    database (3.21%)         list (2.27%)   question (2.10%)   \n",
            "31       named (3.12%)         with (2.85%)          , (2.07%)   \n",
            "32           \" (3.49%)         Data (2.92%)    Student (2.66%)   \n",
            "33        with (5.98%)            . (4.44%)    Profile (3.47%)   \n",
            "34       holds (5.76%)   represents (4.13%)      inher (3.46%)   \n",
            "35         two (4.69%)         many (4.48%)       some (3.05%)   \n",
            "36        User (5.09%)          few (4.27%)     string (3.65%)   \n",
            "37         for (2.33%)         User (1.29%)          , (0.51%)   \n",
            "38    Username (4.24%)           Id (2.96%)         ID (2.55%)   \n",
            "39     Details (0.87%)       Status (0.82%)       Type (0.79%)   \n",
            "40           , (7.31%)            ( (1.38%)         \\n (1.07%)   \n",
            "41             (3.80%)         When (3.28%)         In (2.72%)   \n",
            "42        need (6.02%)        would (5.89%)       then (1.15%)   \n",
            "43        able (3.55%)       having (2.48%)   creating (2.43%)   \n",
            "44      Entity (3.84%)          LIN (2.94%)        Lin (2.89%)   \n",
            "45         Flu (2.70%)         code (2.47%)        Add (1.97%)   \n",
            "46     Manager (2.42%)       object (2.12%)      table (2.07%)   \n",
            "47       field (2.95%)       column (1.81%)        for (1.09%)   \n",
            "48          of (2.55%)         from (1.58%)         on (1.38%)   \n",
            "49    identify (3.41%)      display (2.66%)      check (2.60%)   \n",
            "\n",
            "             Choice 8            Choice 9  ...               Choice 91  \\\n",
            "0          my (0.71%)          as (0.65%)  ...   independently (0.01%)   \n",
            "1    creating (1.07%)    building (0.70%)  ...       providing (0.04%)   \n",
            "2        site (1.05%)      simple (1.03%)  ...            form (0.14%)   \n",
            "3        with (4.79%)           , (2.67%)  ...           again (0.02%)   \n",
            "4       needs (2.84%)       would (1.16%)  ...           shows (0.07%)   \n",
            "5       using (1.48%)          us (1.47%)  ...           heavy (0.09%)   \n",
            "6        very (1.07%)      number (1.07%)  ...            site (0.16%)   \n",
            "7          on (0.04%)         and (0.04%)  ...              up (0.00%)   \n",
            "8        work (0.97%)     testing (0.90%)  ...        analysis (0.18%)   \n",
            "9        from (1.97%)        that (1.74%)  ...         capture (0.09%)   \n",
            "10        run (0.32%)       build (0.29%)  ...          return (0.03%)   \n",
            "11   imported (2.48%)      pulled (2.08%)  ...           drawn (0.16%)   \n",
            "12         as (1.25%)           ( (0.88%)  ...             ... (0.02%)   \n",
            "13      Mongo (1.40%)       MySQL (1.39%)  ...              AD (0.05%)   \n",
            "14   specific (1.25%)          DB (1.07%)  ...       directory (0.08%)   \n",
            "15         in (0.99%)          on (0.62%)  ...           about (0.01%)   \n",
            "16        For (2.12%)          In (2.10%)  ...           Hence (0.05%)   \n",
            "17        've (1.92%)        will (1.28%)  ...            work (0.04%)   \n",
            "18     having (2.14%)    thinking (2.06%)  ...      collecting (0.07%)   \n",
            "19          C (2.58%)          an (2.07%)  ...               E (0.11%)   \n",
            "20       code (1.28%)    database (1.21%)  ...         Android (0.15%)   \n",
            "21       Fram (0.11%)       Frame (0.10%)  ...          design (0.00%)   \n",
            "22       Code (3.40%)        with (3.02%)  ...            migr (0.04%)   \n",
            "23   generate (2.80%)      manage (2.57%)  ...        transfer (0.05%)   \n",
            "24        all (2.37%)        that (1.62%)  ...       databases (0.00%)   \n",
            "25      model (0.22%)      entity (0.13%)  ...         desired (0.01%)   \n",
            "26         as (0.94%)          to (0.76%)  ...          stored (0.01%)   \n",
            "27         In (1.98%)          It (1.47%)  ...        Database (0.03%)   \n",
            "28    created (1.21%)        know (0.97%)  ...      downloaded (0.03%)   \n",
            "29       read (1.70%)          to (1.57%)  ...           these (0.09%)   \n",
            "30       view (2.06%)      method (1.73%)  ...         Product (0.12%)   \n",
            "31         to (1.84%)          in (1.70%)  ...         created (0.04%)   \n",
            "32     Entity (1.52%)           ' (1.45%)  ...               : (0.16%)   \n",
            "33       Data (2.03%)       where (1.27%)  ...      containing (0.06%)   \n",
            "34       will (3.16%)      stores (2.62%)  ...           seems (0.03%)   \n",
            "35        all (2.37%)     several (1.70%)  ...        entities (0.03%)   \n",
            "36      bunch (2.32%)      couple (1.92%)  ...            bool (0.08%)   \n",
            "37          \" (0.45%)       which (0.35%)  ...          called (0.01%)   \n",
            "38    Address (2.32%)          id (1.36%)  ...            role (0.09%)   \n",
            "39       Role (0.70%)     Profile (0.64%)  ...           Score (0.02%)   \n",
            "40       with (0.79%)           : (0.57%)  ...              is (0.01%)   \n",
            "41         My (1.76%)          It (1.46%)  ...     Furthermore (0.03%)   \n",
            "42        use (0.96%)         can (0.94%)  ...            show (0.02%)   \n",
            "43        not (2.01%)   wondering (1.72%)  ...       following (0.06%)   \n",
            "44         EF (2.09%)         Flu (0.98%)  ...             its (0.04%)   \n",
            "45         EF (1.77%)        Code (1.54%)  ...           query (0.16%)   \n",
            "46     entity (1.70%)          Id (1.43%)  ...         Details (0.04%)   \n",
            "47       from (0.84%)   attribute (0.67%)  ...           based (0.01%)   \n",
            "48    because (1.12%)          so (0.91%)  ...           above (0.01%)   \n",
            "49        get (2.28%)      lookup (1.70%)  ...            list (0.09%)   \n",
            "\n",
            "               Choice 92             Choice 93              Choice 94  \\\n",
            "0      backwards (0.01%)    completely (0.01%)           your (0.01%)   \n",
            "1         having (0.04%)          many (0.04%)      replacing (0.04%)   \n",
            "2           task (0.14%)             P (0.13%)         legacy (0.13%)   \n",
            "3           full (0.02%)     designing (0.02%)             my (0.02%)   \n",
            "4        heavily (0.07%)       relates (0.07%)        started (0.07%)   \n",
            "5           code (0.09%)         login (0.09%)          MySQL (0.09%)   \n",
            "6       variable (0.15%)        person (0.15%)        special (0.15%)   \n",
            "7              \" (0.00%)             3 (0.00%)      computing (0.00%)   \n",
            "8      searching (0.18%)   computation (0.18%)            CSS (0.17%)   \n",
            "9        editing (0.09%)     regarding (0.09%)         format (0.08%)   \n",
            "10           the (0.03%)          save (0.03%)       simulate (0.03%)   \n",
            "11        aggreg (0.16%)      received (0.16%)   synchronized (0.15%)   \n",
            "12          long (0.02%)       against (0.02%)         around (0.02%)   \n",
            "13            db (0.05%)             J (0.05%)             as (0.05%)   \n",
            "14        common (0.08%)            My (0.08%)      Microsoft (0.08%)   \n",
            "15       program (0.01%)            we (0.01%)    efficiently (0.01%)   \n",
            "16          Even (0.05%)           Its (0.05%)         Rather (0.05%)   \n",
            "17        simply (0.04%)         began (0.04%)         always (0.04%)   \n",
            "18    generating (0.07%)          quer (0.07%)        sending (0.07%)   \n",
            "19     Cassandra (0.11%)       Android (0.11%)           JSON (0.11%)   \n",
            "20         basic (0.15%)        method (0.15%)           Cake (0.15%)   \n",
            "21        Reader (0.00%)       library (0.00%)           Tool (0.00%)   \n",
            "22         built (0.04%)         where (0.04%)             at (0.04%)   \n",
            "23           SQL (0.05%)          link (0.05%)           open (0.05%)   \n",
            "24             / (0.00%)            on (0.00%)          users (0.00%)   \n",
            "25      incoming (0.01%)           big (0.01%)    persistence (0.01%)   \n",
            "26        during (0.01%)             / (0.01%)    efficiently (0.01%)   \n",
            "27          Does (0.03%)        Within (0.03%)           Even (0.03%)   \n",
            "28       require (0.03%)        looked (0.03%)           hope (0.03%)   \n",
            "29        almost (0.09%)     currently (0.09%)        checked (0.09%)   \n",
            "30        Master (0.12%)        public (0.12%)        complex (0.12%)   \n",
            "31          Item (0.04%)             ( (0.04%)         System (0.04%)   \n",
            "32         House (0.16%)      Contract (0.16%)              ` (0.16%)   \n",
            "33       Address (0.05%)            DB (0.05%)           Cont (0.05%)   \n",
            "34        serves (0.03%)       follows (0.03%)          deals (0.03%)   \n",
            "35             , (0.03%)             ( (0.03%)         stored (0.03%)   \n",
            "36             P (0.08%)        person (0.08%)          three (0.08%)   \n",
            "37         Named (0.01%)       Private (0.01%)            For (0.01%)   \n",
            "38        Author (0.09%)      Security (0.09%)         Emails (0.09%)   \n",
            "39          Prof (0.02%)            Us (0.02%)         String (0.02%)   \n",
            "40     contained (0.01%)         being (0.01%)         inside (0.01%)   \n",
            "41          Most (0.03%)        Please (0.03%)           Next (0.03%)   \n",
            "42          dont (0.02%)         saved (0.02%)        noticed (0.01%)   \n",
            "43        simply (0.06%)       mapping (0.06%)        opening (0.06%)   \n",
            "44             2 (0.04%)             c (0.04%)              L (0.04%)   \n",
            "45    repository (0.15%)            As (0.15%)      namespace (0.15%)   \n",
            "46      instance (0.04%)    repository (0.04%)           View (0.04%)   \n",
            "47            ID (0.01%)            id (0.01%)       together (0.01%)   \n",
            "48   extensively (0.01%)          back (0.01%)    dynamically (0.00%)   \n",
            "49       encrypt (0.08%)   distinguish (0.08%)         manage (0.08%)   \n",
            "\n",
            "                Choice 95            Choice 96           Choice 97  \\\n",
            "0    specifically (0.01%)        daily (0.01%)        upon (0.01%)   \n",
            "1              to (0.04%)            , (0.04%)           : (0.04%)   \n",
            "2          server (0.13%)       report (0.13%)   prototype (0.13%)   \n",
            "3     integrating (0.02%)    featuring (0.02%)   analyzing (0.02%)   \n",
            "4           reads (0.07%)         need (0.07%)     involve (0.06%)   \n",
            "5     interacting (0.09%)      complex (0.09%)     Windows (0.09%)   \n",
            "6           query (0.15%)     complete (0.15%)     project (0.15%)   \n",
            "7        accuracy (0.00%)        speed (0.00%)        each (0.00%)   \n",
            "8               R (0.17%)         very (0.17%)      system (0.16%)   \n",
            "9          import (0.08%)         over (0.08%)    security (0.08%)   \n",
            "10          start (0.03%)    implement (0.03%)       fully (0.03%)   \n",
            "11         mapped (0.15%)      plotted (0.15%)     tracked (0.15%)   \n",
            "12         safely (0.02%)         then (0.02%)       graph (0.02%)   \n",
            "13      variables (0.05%)            E (0.05%)      binary (0.05%)   \n",
            "14         couple (0.07%)      special (0.07%)      remote (0.07%)   \n",
            "15       together (0.01%)   containing (0.01%)     located (0.01%)   \n",
            "16       Database (0.05%)         Over (0.05%)     Luckily (0.05%)   \n",
            "17     previously (0.04%)         then (0.04%)          \\n (0.03%)   \n",
            "18      receiving (0.07%)     actually (0.06%)    confused (0.06%)   \n",
            "19             As (0.10%)            N (0.10%)        fire (0.10%)   \n",
            "20              c (0.15%)           PD (0.15%)     command (0.15%)   \n",
            "21              2 (0.00%)    component (0.00%)        core (0.00%)   \n",
            "22             To (0.04%)         here (0.04%)         dat (0.04%)   \n",
            "23           give (0.05%)          des (0.05%)         pre (0.05%)   \n",
            "24         schema (0.00%)      queries (0.00%)      client (0.00%)   \n",
            "25             EF (0.01%)        store (0.01%)      client (0.01%)   \n",
            "26            due (0.01%)           it (0.01%)       based (0.01%)   \n",
            "27          Later (0.03%)       Though (0.03%)     Instead (0.03%)   \n",
            "28   successfully (0.03%)         dont (0.02%)        work (0.02%)   \n",
            "29     simplified (0.09%)       loaded (0.09%)   succeeded (0.09%)   \n",
            "30            one (0.11%)      mapping (0.11%)      series (0.11%)   \n",
            "31            Car (0.04%)         type (0.04%)       class (0.04%)   \n",
            "32       Computer (0.15%)       Author (0.15%)        Site (0.15%)   \n",
            "33         having (0.05%)        Query (0.05%)          DA (0.05%)   \n",
            "34          comes (0.02%)        which (0.02%)       class (0.02%)   \n",
            "35           Date (0.03%)            6 (0.03%)      unique (0.03%)   \n",
            "36            has (0.08%)         fore (0.08%)      static (0.08%)   \n",
            "37           Item (0.01%)     entitled (0.01%)           @ (0.01%)   \n",
            "38         Groups (0.09%)        phone (0.09%)    nickname (0.09%)   \n",
            "39          Agent (0.02%)         Pass (0.02%)         and (0.02%)   \n",
            "40         within (0.01%)       String (0.01%)          at (0.01%)   \n",
            "41           Just (0.03%)          the (0.02%)       Where (0.02%)   \n",
            "42         expect (0.01%)        fetch (0.01%)   generated (0.01%)   \n",
            "43      attaching (0.06%)     confused (0.06%)   declaring (0.05%)   \n",
            "44            Get (0.04%)       tables (0.04%)          to (0.04%)   \n",
            "45         simple (0.15%)         name (0.15%)    Property (0.15%)   \n",
            "46           Form (0.04%)            D (0.03%)       Entry (0.03%)   \n",
            "47      retrieved (0.01%)            - (0.01%)      Column (0.01%)   \n",
            "48          there (0.00%)      between (0.00%)        many (0.00%)   \n",
            "49      calculate (0.08%)         auto (0.07%)        know (0.07%)   \n",
            "\n",
            "                Choice 98               Choice 99              Choice 100  \n",
            "0        overtime (0.01%)            both (0.01%)         between (0.01%)  \n",
            "1           large (0.03%)         Android (0.03%)    establishing (0.03%)  \n",
            "2              \\n (0.13%)            test (0.13%)             map (0.12%)  \n",
            "3    implementing (0.02%)       targeting (0.02%)          having (0.02%)  \n",
            "4           links (0.06%)           doesn (0.06%)        receives (0.06%)  \n",
            "5         passing (0.08%)               : (0.08%)          taking (0.08%)  \n",
            "6          strong (0.15%)          change (0.15%)           model (0.15%)  \n",
            "7             web (0.00%)            over (0.00%)        updating (0.00%)  \n",
            "8          jQuery (0.16%)            JSON (0.16%)   documentation (0.16%)  \n",
            "9         merging (0.08%)               - (0.08%)            flow (0.08%)  \n",
            "10          match (0.03%)            form (0.02%)         compile (0.02%)  \n",
            "11         merged (0.15%)      translated (0.15%)         handled (0.14%)  \n",
            "12         either (0.01%)   automatically (0.01%)           there (0.01%)  \n",
            "13         Amazon (0.05%)           Couch (0.05%)               X (0.05%)  \n",
            "14          fixed (0.07%)        scalable (0.07%)      dictionary (0.07%)  \n",
            "15        backend (0.01%)          inside (0.01%)               … (0.01%)  \n",
            "16     Previously (0.05%)          Having (0.05%)           Which (0.05%)  \n",
            "17        planned (0.03%)        normally (0.03%)      downloaded (0.03%)  \n",
            "18          fetch (0.06%)       importing (0.06%)        supposed (0.06%)  \n",
            "19            Dat (0.10%)               Z (0.10%)             Del (0.10%)  \n",
            "20         Amazon (0.14%)              OD (0.14%)             IBM (0.14%)  \n",
            "21           from (0.00%)              Di (0.00%)               B (0.00%)  \n",
            "22              m (0.04%)      repository (0.04%)         context (0.04%)  \n",
            "23           gain (0.05%)         process (0.05%)            push (0.05%)  \n",
            "24       customer (0.00%)         strings (0.00%)          simple (0.00%)  \n",
            "25           date (0.01%)         complex (0.01%)        Database (0.01%)  \n",
            "26            set (0.01%)       retrieved (0.01%)         created (0.01%)  \n",
            "27            Not (0.03%)             Its (0.03%)             Two (0.03%)  \n",
            "28          build (0.02%)          simply (0.02%)        generate (0.02%)  \n",
            "29       provided (0.09%)     encountered (0.09%)          posted (0.09%)  \n",
            "30           good (0.11%)          search (0.11%)           doubt (0.11%)  \n",
            "31           Home (0.04%)            View (0.04%)            here (0.04%)  \n",
            "32        Message (0.15%)        Students (0.15%)            Blog (0.15%)  \n",
            "33           Page (0.05%)               - (0.05%)              ID (0.05%)  \n",
            "34          every (0.02%)         records (0.02%)           wants (0.02%)  \n",
            "35        foreign (0.03%)      collection (0.03%)             and (0.03%)  \n",
            "36              1 (0.08%)               [ (0.07%)   corresponding (0.07%)  \n",
            "37              _ (0.01%)              US (0.01%)              so (0.01%)  \n",
            "38        Members (0.09%)            Tags (0.08%)               e (0.08%)  \n",
            "39          Title (0.02%)           Index (0.02%)               , (0.02%)  \n",
            "40              & (0.01%)             And (0.01%)         through (0.01%)  \n",
            "41           Lets (0.02%)           Under (0.02%)         Instead (0.02%)  \n",
            "42     originally (0.01%)           build (0.01%)            take (0.01%)  \n",
            "43    registering (0.05%)            very (0.05%)          giving (0.05%)  \n",
            "44         string (0.04%)             Web (0.04%)               I (0.04%)  \n",
            "45          Login (0.14%)             One (0.14%)           Gener (0.14%)  \n",
            "46            Nav (0.03%)            Base (0.03%)             Add (0.03%)  \n",
            "47       manually (0.01%)         feature (0.01%)             int (0.01%)  \n",
            "48         mainly (0.00%)       alongside (0.00%)            even (0.00%)  \n",
            "49       navigate (0.07%)          locate (0.07%)            name (0.07%)  \n",
            "\n",
            "[50 rows x 101 columns]\n"
          ]
        }
      ],
      "source": [
        "starting_prompt = \"I am working\"\n",
        "# Encode input text to PyTorch tensors.\n",
        "input_tokens = tokenizer.encode(starting_prompt, return_tensors=\"pt\").to(device) #return_tensors=\"pt\"= tokens will be represented as tensors\n",
        "iterations = []\n",
        "num_generation_steps = 50\n",
        "num_choices_per_step = 100\n",
        "# Generate text iteratively\n",
        "with torch.no_grad():\n",
        "    for step in range(num_generation_steps):\n",
        "\n",
        "        # Create a dictionary for current iteration data\n",
        "        current_iteration = {\"Input\": tokenizer.decode(input_tokens[0])}\n",
        "\n",
        "        # Generate token probabilities and sort for best choices\n",
        "        model_output = model(input_ids=input_tokens)\n",
        "        next_token_probabilities = torch.softmax(model_output.logits[0, -1, :], dim=-1)\n",
        "        top_token_ids = torch.argsort(next_token_probabilities, descending=True)\n",
        "\n",
        "        # Store top choices with probabilities\n",
        "        for choice_index in range(num_choices_per_step):\n",
        "            token_id = top_token_ids[choice_index].item()\n",
        "            token_probability = next_token_probabilities[token_id].item()\n",
        "\n",
        "            token_choice = f\"{tokenizer.decode(token_id)} ({100 * token_probability:.2f}%)\"\n",
        "            current_iteration[f\"Choice {choice_index+1}\"] = token_choice\n",
        "\n",
        "        # Append the most likely token to input\n",
        "        input_tokens = torch.cat([input_tokens, top_token_ids[None, 0, None]], dim=-1)\n",
        "\n",
        "        # Add iteration data to list\n",
        "        iterations.append(current_iteration)\n",
        "# Create a DataFrame for clear presentation\n",
        "output_df = pd.DataFrame(iterations)\n",
        "print(output_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLo3teRV-A2y",
        "outputId": "1bbcc27d-43c4-41a2-8983-38aeeb4bec5f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am working on a project that requires a lot of data to be stored in a database. I am using the Entity Framework to store the data. I have a class called User that has a property called UserName. I am using the UserName property to store\n"
          ]
        }
      ],
      "source": [
        "# Encode the starting prompt into input tensors and attention mask tensors\n",
        "input_ids = tokenizer(starting_prompt, return_tensors=\"pt\").input_ids.to(device) # Encoding input into PyTorch tensors\n",
        "attention_mask = tokenizer(starting_prompt, return_tensors=\"pt\").attention_mask.to(device) # Attention mask for input\n",
        "\n",
        "# Generate text using the language model\n",
        "output = model.generate(\n",
        "    input_ids=input_ids, # Provide input tensors\n",
        "    attention_mask=attention_mask, # Provide attention mask tensors\n",
        "    max_new_tokens=num_generation_steps, # Maximum number of tokens to generate\n",
        "    do_sample=False # Disable sampling to get deterministic output\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(tokenizer.decode(output[0])) # Decode the generated tokens and print the resulting text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwu5OQEh-sFN"
      },
      "source": [
        "# Greedy Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxwK5Yrz-s6T"
      },
      "outputs": [],
      "source": [
        "max_length = 120\n",
        "# Define the input text\n",
        "input_txt = \"\"\"The ancient oracle foretold a hero rising from the ashes, but \\\n",
        "only the whispers of forgotten magic offer clues. \\\n",
        "Where does the hero's journey begin?\\n\\n\n",
        "\"\"\"\n",
        "\n",
        "# Encode the input text into input tensors\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device) # Encode input into PyTorch tensors\n",
        "\n",
        "# Generate text using greedy decoding with a maximum length constraint\n",
        "output_greedy = model.generate(\n",
        "    input_ids, # Provide input tensors\n",
        "    max_length=max_length, # Set the maximum length for the generated text\n",
        "    do_sample=False, # Disable sampling to use greedy decoding\n",
        "    pad_token_id=tokenizer.eos_token_id # Set the pad token ID to handle sequences shorter than max_length\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(tokenizer.decode(output_greedy[0])) # Decode the generated tokens and print the resulting text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsAfKnK2-98m"
      },
      "source": [
        "# Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3CGmB-P-9W5",
        "outputId": "67a51a7f-380c-487a-b623-0f895f9d1178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ancient oracle foretold a hero rising from the ashes, but only the whispers of forgotten magic offer clues. Where does the hero's journey begin?\n",
            "\n",
            "\n",
            "\n",
            "The hero’s journey is the journey of a man or a woman, a child or an adult, from one place to another. It is a journey that begins with a single step and ends in a new place. The hero is not the one who takes the first step. He or she is simply the person who has taken the next step, and so on, until they reach their destination.\n",
            "\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "# Generate text using beam search decoding with specified parameters\n",
        "output_beam = model.generate(\n",
        "    input_ids,  # Provide input tensors\n",
        "    max_length=max_length,  # Set the maximum length for the generated text\n",
        "    num_beams=3,  # Set the number of beams for beam search decoding\n",
        "    do_sample=False,  # Disable sampling to use beam search decoding\n",
        "    no_repeat_ngram_size=2,  # Set the size of the n-grams to avoid repetition\n",
        "    pad_token_id=tokenizer.eos_token_id  # Set the pad token ID to handle sequences shorter than max_length\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(tokenizer.decode(output_beam[0]))  # Decode the generated tokens and print the resulting text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGq7Z7i7_FNE"
      },
      "outputs": [],
      "source": [
        "# Generate text using beam search decoding with specified parameters\n",
        "output_beam = model.generate(\n",
        "    input_ids,  # Provide input tensors\n",
        "    max_length=max_length,  # Set the maximum length for the generated text\n",
        "    num_beams=5,  # Set the number of beams for beam search decoding\n",
        "    do_sample=False,  # Disable sampling to use beam search decoding\n",
        "    no_repeat_ngram_size=2,  # Set the size of the n-grams to avoid repetition\n",
        "    pad_token_id=tokenizer.eos_token_id  # Set the pad token ID to handle sequences shorter than max_length\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(tokenizer.decode(output_beam[0]))  # Decode the generated tokens and print the resulting text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abCqUSQw_LIg"
      },
      "source": [
        "# Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNZxWQ35_MY7"
      },
      "outputs": [],
      "source": [
        "# Generate text using sampling with temperature scaling\n",
        "output_temp = model.generate(\n",
        "    input_ids,  # Provide input tensors\n",
        "    max_length=max_length,  # Set the maximum length for the generated text\n",
        "    do_sample=True,  # Enable sampling to generate diverse outputs\n",
        "    temperature=2.0,  # Set the temperature parameter for temperature scaling\n",
        "    top_k=0,  # Set top_k to 0 to allow all tokens to be considered during sampling\n",
        "    pad_token_id=tokenizer.eos_token_id  # Set the pad token ID to handle sequences shorter than max_length\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(tokenizer.decode(output_temp[0]))  # Decode the generated tokens and print the resulting text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYgT93A4_P7G",
        "outputId": "234c5671-2d0e-424d-95e8-ece1567cb0af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ancient oracle foretold a hero rising from the ashes, but only the whispers of forgotten magic offer clues. Where does the hero's journey begin?\n",
            "\n",
            "\n",
            "\n",
            "The Oracle of Delphi was the last great oracle of ancient Greece, and the first to warn of the coming of a new hero. But the Oracle never told of the hero, or his name. Now, the Oracle of Delphi is the last oracle of magic, and the first to tell of the hero’s journey.\n",
            "\n",
            "\n",
            "\n",
            "The Oracle of Delphi first appeared in the 4th century BC,\n"
          ]
        }
      ],
      "source": [
        "# Generate text using sampling with temperature scaling\n",
        "output_temp = model.generate(\n",
        "    input_ids,  # Provide input tensors\n",
        "    max_length=max_length,  # Set the maximum length for the generated text\n",
        "    do_sample=True,  # Enable sampling to generate diverse outputs\n",
        "    temperature=0.6,  # Set the temperature parameter for temperature scaling (lower temperature results in more conservative sampling)\n",
        "    top_k=0,  # Set top_k to 0 to allow all tokens to be considered during sampling\n",
        "    pad_token_id=tokenizer.eos_token_id  # Set the pad token ID to handle sequences shorter than max_length\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(tokenizer.decode(output_temp[0]))  # Decode the generated tokens and print the resulting text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vbLAUr8_Vsi"
      },
      "source": [
        "# Top-k and Top-p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TvCf8Ps_aj8"
      },
      "source": [
        "**Setting Top K**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-K7vfop_W9z",
        "outputId": "b74c6dd4-4028-40c9-fca4-1eeaac333bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ancient oracle foretold a hero rising from the ashes, but only the whispers of forgotten magic offer clues. Where does the hero's journey begin?\n",
            "\n",
            "\n",
            "\n",
            "We've been talking a lot about how modern, western society is destroying this land since the early 21st century. But now it seems that we can only see it from the perspective of the future. The ancient oracle of Endor foretold of a hero rising from the ashes.\n",
            "\n",
            "\n",
            "\n",
            "What happens when the heroes' path crosses with a world once created by magic? Perhaps when they are faced with an ancient prophecy and\n"
          ]
        }
      ],
      "source": [
        "# Generate text using sampling with top-k sampling\n",
        "out_top_k = model.generate(\n",
        "    input_ids,  # Provide input tensors\n",
        "    max_length=max_length,  # Set the maximum length for the generated text\n",
        "    do_sample=True,  # Enable sampling to generate diverse outputs\n",
        "    top_k=40,  # Set the top_k parameter for top-k sampling (select from top 40 tokens based on logits)\n",
        "    pad_token_id=tokenizer.eos_token_id  # Set the pad token ID to handle sequences shorter than max_length\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(tokenizer.decode(out_top_k[0]))  # Decode the generated tokens and print the resulting text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71ZjR3BB_gn3"
      },
      "source": [
        "**Setting Top P**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-kpBwo__hrY",
        "outputId": "eeb552bd-8454-4051-99b1-49883864016a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ancient oracle foretold a hero rising from the ashes, but only the whispers of forgotten magic offer clues. Where does the hero's journey begin?\n",
            "\n",
            "\n",
            "\n",
            "The prophecy was about the hero.\n",
            "\n",
            "A great army of heroes had marched out of the dark days of the world. They were a force to be reckoned with. They would destroy the forces of darkness and rise from the ashes of the earth. The army was strong, but the path to glory lay in the hands of the true heroes. The heroes of the ancient prophecy were not the men and women that we are today.\n"
          ]
        }
      ],
      "source": [
        "# Generate text using sampling with nucleus sampling\n",
        "out_topp = model.generate(\n",
        "    input_ids,  # Provide input tensors\n",
        "    max_length=max_length,  # Set the maximum length for the generated text\n",
        "    do_sample=True,  # Enable sampling to generate diverse outputs\n",
        "    top_p=0.80,  # Set the top_p parameter for nucleus sampling (select from the smallest set of tokens whose cumulative probability exceeds p)\n",
        "    pad_token_id=tokenizer.eos_token_id  # Set the pad token ID to handle sequences shorter than max_length\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(tokenizer.decode(out_topp[0]))  # Decode the generated tokens and print the resulting text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZh6znQl_liP"
      },
      "source": [
        "**Setting Top P and Top K**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TSM9cfcJ_k-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba1cc2a-3c18-49e6-90de-f53bb34a7eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ancient oracle foretold a hero rising from the ashes, but only the whispers of forgotten magic offer clues. Where does the hero's journey begin?\n",
            "\n",
            "\n",
            "\n",
            "I have no words to describe the feeling I had when I saw that the great temple was not only a monument to the ancient world, but a museum to the gods themselves.\n",
            "\n",
            "A monument to the ancient world\n",
            "\n",
            "I'm sure most of us have a vague memory of our childhood trips to our local town's temple, and it was at the temple that we were taught about the gods, and the sacred history behind the\n"
          ]
        }
      ],
      "source": [
        "# Generate text using sampling with top-k and nucleus sampling\n",
        "out_top_pk = model.generate(\n",
        "    input_ids,  # Provide input tensors\n",
        "    max_length=max_length,  # Set the maximum length for the generated text\n",
        "    do_sample=True,  # Enable sampling to generate diverse outputs\n",
        "    top_k=40,  # Set the top_k parameter for top-k sampling (select from top 40 tokens based on logits)\n",
        "    top_p=0.80,  # Set the top_p parameter for nucleus sampling (select from the smallest set of tokens whose cumulative probability exceeds p)\n",
        "    pad_token_id=tokenizer.eos_token_id  # Set the pad token ID to handle sequences shorter than max_length\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "print(tokenizer.decode(out_top_pk[0]))  # Decode the generated tokens and print the resulting text\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm2vJT+oO/Yf8Y5EQGzy5T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}